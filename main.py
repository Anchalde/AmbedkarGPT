"""
AmbedkarGPT - AI Intern Assignment (Kalpit Pvt Ltd)
Author: Nick (Maneesh Reddy Alugupalli)

A clean, warning-free LangChain RAG prototype using:
 - HuggingFace embeddings (sentence-transformers/all-MiniLM-L6-v2)
 - ChromaDB (local vector store)
 - Ollama (Mistral 7B LLM)
"""

import os
import warnings
import logging

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Silence unwanted warnings/logs
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", category=UserWarning)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["ANONYMIZED_TELEMETRY"] = "false"
logging.getLogger("chromadb.telemetry").setLevel(logging.ERROR)
logging.getLogger("chromadb").setLevel(logging.ERROR)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LangChain Imports
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA


def build_vector_store(file_path: str, persist_directory: str = "db"):
    """Load text, split into chunks, create embeddings, and store in ChromaDB."""
    print("\nğŸ“˜ Loading document...")
    loader = TextLoader(file_path)
    documents = loader.load()

    print("âœ‚ï¸ Splitting document into chunks...")
    splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=300,
        chunk_overlap=50,
        length_function=len
    )
    docs = splitter.split_documents(documents)

    print("ğŸ§  Creating embeddings using HuggingFace...")
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

    print("ğŸ’¾ Storing vectors locally in ChromaDB...")
    vectorstore = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)

    return vectorstore


def create_qa_chain(vectorstore):
    """Create RetrievalQA chain using Ollamaâ€™s Mistral 7B."""
    print("\nâš™ï¸ Initializing Ollama (Mistral 7B)...")
    llm = Ollama(model="mistral")

    print("ğŸ” Setting up RetrievalQA pipeline...")
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 2}),
        return_source_documents=True
    )
    return qa


def main():
    persist_dir = "db"
    file_path = "speech.txt"

    # Load or build vector store
    if not os.path.exists(persist_dir):
        vectorstore = build_vector_store(file_path, persist_dir)
    else:
        print("\nğŸ“¦ Loading existing ChromaDB store...")
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore = Chroma(persist_directory=persist_dir, embedding_function=embeddings)

    qa_chain = create_qa_chain(vectorstore)

    print("\nâœ… Setup complete. You can now ask questions!")
    print("---------------------------------------------------")

    while True:
        query = input("\nâ“ Ask a question (or type 'exit' to quit): ").strip()
        if query.lower() == "exit":
            print("ğŸ‘‹ Exiting. Goodbye!")
            break

        # Use invoke() instead of deprecated __call__()
        result = qa_chain.invoke({"query": query})

        print("\nğŸ’¬ Answer:")
        print(result["result"])

        print("\nğŸ“„ Source context:")
        for doc in result["source_documents"]:
            print("-", doc.page_content.strip()[:120], "...\n")


if __name__ == "__main__":
    main()
